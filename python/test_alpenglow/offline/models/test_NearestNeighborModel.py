import alpenglow as prs
from alpenglow.offline.models import NearestNeighborModel
from alpenglow.offline.evaluation import NdcgScore
import alpenglow.Getter as rs
import pandas as pd
import numpy as np
import unittest
import sys
import pytest
import alpenglow.cpp
compiler = alpenglow.cpp.__compiler
stdlib = alpenglow.cpp.__stdlib


class TestNearestNeighborModel(unittest.TestCase):
    def test_rmse(self):
        data = pd.read_csv(
            "python/test_alpenglow/test_data_4",
            sep=' ',
            header=None,
            names=['time', 'user', 'item', 'id', 'score', 'eval']
        )
        model = NearestNeighborModel()
        model.fit(data)

        def predict(model, user, item):
            rd = rs.RecDat()
            rd.user = user
            rd.item = item
            return model.prediction(rd)

        errors = [(1 - predict(model.model, u, i))**2 for (u, i) in data[['user', 'item']].values]
        rmse = np.sqrt(pd.Series(errors)).mean()
        assert rmse == pytest.approx(53.651, abs=0.5)

    def test_ranking(self):
        data = pd.read_csv(
            "python/test_alpenglow/test_data_4",
            sep=' ',
            header=None,
            names=['time', 'user', 'item', 'id', 'score', 'eval']
        )
        exp = NearestNeighborModel()
        exp.fit(data)
        preds = exp.recommend(exclude_known=False, k=20)

        print("toplist="+str(preds['item'].tolist()))

        if(compiler == "gcc" and stdlib == "libstdc++"):
            assert preds['item'].tolist() ==  \
                [94, 166, 225, 300, 30, 299, 455, 337, 250, 338, 383, 452, 256, 98, 293, 260, 462, 168, 120, 128, 94, 166, 30, 225, 442, 98, 300, 372, 337, 299, 462, 455, 256, 196, 338, 247, 128, 177, 102, 62, 94, 166, 225, 30, 98, 300, 442, 299, 372, 337, 462, 455, 250, 496, 196, 429, 338, 427, 256, 371, 30, 166, 94, 225, 300, 337, 372, 299, 462, 196, 383, 452, 256, 98, 293, 260, 455, 168, 128, 4, 30, 300, 442, 166, 225, 94, 462, 204, 299, 250, 383, 452, 256, 98, 293, 260, 455, 168, 120, 128, 166, 30, 225, 94, 300, 442, 462, 98, 372, 299, 196, 337, 128, 165, 455, 383, 204, 256, 86, 247, 30, 94, 166, 225, 442, 300, 196, 462, 299, 204, 98, 455, 372, 256, 337, 250, 86, 128, 293, 452, 225, 94, 166, 442, 300, 337, 455, 372, 98, 30, 299, 383, 452, 256, 293, 260, 462, 168, 128, 4, 166, 94, 225, 30, 300, 299, 98, 442, 337, 372, 455, 62, 196, 250, 462, 215, 40, 338, 496, 86, 94, 166, 30, 225, 442, 372, 462, 337, 98, 338, 300, 256, 299, 455, 120, 128, 247, 293, 452, 260, 94, 225, 166, 30, 442, 299, 455, 98, 337, 300, 372, 462, 196, 338, 383, 293, 452, 256, 260, 168, 30, 94, 225, 166, 300, 442, 462, 299, 196, 337, 372, 98, 250, 455, 256, 204, 40, 400, 165, 293, 94, 166, 30, 225, 300, 299, 442, 372, 98, 337, 462, 455, 196, 62, 204, 40, 250, 383, 293, 452, 94, 166, 225, 30, 442, 300, 98, 299, 372, 462, 337, 196, 455, 429, 250, 371, 296, 122, 256, 409, 94, 30, 166, 442, 225, 98, 462, 196, 455, 299, 383, 452, 256, 260, 168, 120, 293, 128, 204, 4, 225, 94, 299, 166, 442, 337, 300, 455, 372, 98, 383, 452, 256, 462, 168, 120, 293, 260, 128, 4, 94, 225, 30, 166, 300, 98, 442, 299, 462, 455, 372, 247, 337, 196, 256, 128, 338, 102, 250, 383, 94, 30, 300, 166, 225, 299, 455, 250, 442, 462, 337, 98, 338, 196, 204, 86, 247, 383, 293, 452, 94, 30, 166, 225, 300, 299, 98, 442, 337, 372, 455, 462, 196, 62, 250, 215, 204, 338, 40, 496, 94, 30, 225, 166, 300, 299, 98, 442, 337, 372, 455, 462, 250, 62, 338, 429, 371, 427, 383, 293, 94, 166, 225, 30, 300, 98, 442, 299, 337, 372, 196, 455, 462, 250, 496, 40, 204, 156, 62, 429, 225, 166, 300, 299, 94, 30, 442, 98, 372, 337, 462, 455, 196, 86, 128, 204, 496, 427, 215, 383, 94, 225, 166, 30, 442, 300, 299, 98, 462, 372, 337, 196, 455, 204, 250, 496, 62, 215, 256, 247, 94, 166, 225, 30, 98, 442, 300, 299, 372, 337, 462, 338, 256, 455, 250, 120, 496, 40, 427, 97, 94, 30, 166, 225, 442, 300, 299, 372, 337, 98, 455, 62, 462, 196, 128, 247, 338, 40, 86, 383, 94, 166, 225, 30, 442, 300, 299, 372, 98, 462, 337, 455, 256, 247, 429, 196, 102, 427, 36, 496, 94, 166, 30, 225, 442, 98, 462, 196, 299, 455, 256, 300, 128, 372, 247, 293, 452, 383, 260, 168, 94, 225, 30, 166, 442, 300, 372, 337, 299, 98, 462, 455, 196, 338, 383, 165, 247, 62, 128, 293, 225, 94, 166, 442, 300, 337, 455, 372, 98, 30, 299, 383, 452, 256, 293, 260, 462, 168, 128, 4, 94, 30, 166, 442, 225, 98, 462, 196, 455, 299, 383, 452, 256, 260, 168, 120, 293, 128, 204, 4, 30, 94, 166, 225, 299, 196, 462, 300, 442, 98, 337, 372, 455, 40, 250, 383, 293, 452, 256, 260, 166, 225, 30, 98, 300, 94, 442, 299, 337, 372, 247, 455, 462, 196, 256, 86, 177, 102, 128, 204, 94, 30, 299, 166, 98, 442, 225, 62, 300, 337, 372, 40, 496, 196, 250, 215, 383, 293, 452, 256, 30, 94, 225, 166, 300, 299, 98, 442, 337, 372, 462, 455, 250, 62, 338, 496, 196, 429, 371, 383, 94, 300, 225, 166, 30, 299, 98, 442, 455, 337, 372, 462, 196, 204, 250, 86, 247, 108, 156, 450, 166, 94, 30, 225, 442, 299, 372, 337, 300, 98, 462, 455, 62, 196, 338, 256, 120, 496, 293, 452, 94, 30, 166, 225, 442, 98, 300, 337, 299, 372, 462, 455, 196, 338, 256, 62, 156, 250, 215, 120, 94, 166, 30, 225, 300, 98, 442, 455, 299, 196, 462, 372, 337, 204, 250, 256, 247, 156, 86, 128, 94, 166, 30, 225, 300, 442, 299, 98, 372, 462, 337, 455, 196, 496, 250, 429, 427, 338, 36, 215, 94, 166, 30, 225, 299, 455, 442, 98, 462, 300, 196, 250, 337, 338, 383, 293, 452, 256, 260, 168, 225, 166, 30, 94, 299, 442, 98, 372, 300, 337, 462, 455, 196, 256, 120, 293, 452, 260, 383, 128, 94, 30, 299, 166, 442, 98, 300, 337, 372, 225, 62, 40, 196, 383, 293, 452, 256, 260, 455, 462, 225, 94, 299, 166, 442, 337, 300, 455, 372, 98, 383, 452, 256, 462, 168, 120, 293, 260, 128, 4, 94, 30, 166, 225, 442, 299, 300, 98, 337, 372, 462, 196, 496, 455, 250, 338, 62, 40, 215, 256, 94, 30, 225, 166, 442, 299, 98, 300, 372, 337, 462, 496, 40, 62, 338, 256, 455, 215, 36, 120, 94, 225, 30, 442, 299, 300, 98, 462, 337, 372, 383, 452, 256, 260, 455, 168, 293, 128, 166, 4, 94, 30, 225, 166, 300, 442, 98, 299, 372, 462, 337, 196, 455, 204, 250, 256, 156, 38, 86, 338, 94, 30, 225, 166, 300, 98, 442, 299, 372, 462, 337, 455, 196, 496, 250, 256, 62, 247, 215, 338, 166, 300, 94, 30, 442, 225, 372, 256, 196, 337, 98, 452, 383, 260, 293, 455, 462, 168, 128, 4, 94, 225, 30, 166, 442, 300, 299, 372, 462, 337, 98, 455, 196, 256, 204, 250, 36, 338, 496, 293, 94, 30, 225, 166, 442, 300, 372, 299, 98, 337, 462, 455, 62, 429, 247, 196, 338, 383, 165, 496, 30, 225, 94, 166, 442, 372, 98, 337, 300, 455, 256, 120, 452, 383, 260, 462, 293, 128, 4, 468, 299, 442, 94, 225, 30, 166, 98, 300, 337, 372, 462, 455, 62, 383, 293, 452, 256, 260, 168, 128, 94, 166, 30, 225, 300, 98, 442, 372, 299, 337, 462, 455, 256, 247, 204, 338, 196, 128, 86, 102, 94, 30, 166, 225, 300, 442, 462, 299, 372, 98, 337, 196, 455, 204, 256, 86, 62, 165, 496, 128, 94, 30, 166, 225, 442, 98, 299, 300, 372, 337, 462, 338, 455, 496, 250, 256, 196, 40, 36, 120, 30, 300, 442, 166, 225, 94, 462, 204, 299, 250, 383, 452, 256, 98, 293, 260, 455, 168, 120, 128, 225, 94, 299, 166, 442, 337, 300, 455, 372, 98, 383, 452, 256, 462, 168, 120, 293, 260, 128, 4, 30, 166, 98, 225, 94, 300, 442, 372, 299, 337, 62, 462, 455, 427, 496, 383, 293, 452, 256, 260, 94, 166, 225, 300, 30, 299, 455, 337, 250, 338, 383, 452, 256, 98, 293, 260, 462, 168, 120, 128, 30, 94, 225, 98, 300, 166, 299, 442, 372, 337, 462, 455, 256, 102, 128, 427, 496, 293, 452, 383, 225, 30, 300, 299, 94, 337, 166, 442, 98, 462, 372, 455, 250, 338, 383, 293, 452, 256, 260, 168, 94, 225, 30, 442, 299, 300, 98, 462, 337, 372, 383, 452, 256, 260, 455, 168, 293, 128, 166, 4, 94, 30, 225, 166, 300, 442, 299, 98, 337, 372, 455, 462, 196, 62, 204, 250, 86, 247, 256, 168, 94, 30, 166, 225, 442, 299, 98, 300, 337, 372, 462, 455, 196, 250, 496, 338, 40, 62, 215, 156]

        assert NdcgScore(data, preds, top_k=20) == pytest.approx(0.4987901122546444, abs=1e-2)

        preds2 = exp.recommend(users = [1, 2], exclude_known=False)
        assert preds2['user'].unique().tolist() == [1,2]

        preds = exp.recommend(exclude_known=True)
        joined_preds = preds.join(
            data.set_index(['user', 'item']),
            on=['user','item'], how='inner'
        )
        assert len(joined_preds) == 0